{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: File Ingestion and Schema validation**\n",
    "\n",
    "Take any csv/text file of 2+ GB of your choice.\n",
    "\n",
    "Read the file ( Present approach of reading the file )\n",
    "\n",
    "Try different methods of file reading eg: Dask, Modin, Ray, pandas and present your findings in term of computational efficiency\n",
    "\n",
    "Perform basic validation on data columns : eg: remove special character , white spaces from the col name\n",
    "\n",
    "As you already know the schema hence create a YAML file and write the column name in YAML file. --define separator of\n",
    "read and write file, column name in YAML\n",
    "\n",
    "Validate number of columns and column name of ingested file with YAML.\n",
    "\n",
    "Write the file in pipe separated text file (|) in gz format.\n",
    "\n",
    "Create a summary of the file:\n",
    "\n",
    "*  Total number of rows\n",
    "*   total number of columns\n",
    "*   file size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Books Reviews dataset used from kaggle \n",
    "# (10 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2859504349"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.getsize('E:/solo projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week 6/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the date with Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data with Pandas:  24.951806783676147 sec\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "start = time.time()\n",
    "pd_data = pd.read_csv('E:/solo projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week 6/Books_rating.csv')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Read data with Pandas: \",(end-start),\"sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the data with Dask** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data with Dask:  0.010747432708740234 sec\n"
     ]
    }
   ],
   "source": [
    "from dask import dataframe as dd\n",
    "\n",
    "start = time.time()\n",
    "dask_data = dd.read_csv('E:/solo projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week 6/Books_rating.csv')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Read data with Dask: \",(end-start),\"sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the data with Modin and Ray**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32me:\\solo projects\\Data_Glacier_virtual_internship\\Data_Glacier_virtual_internship\\Week 6\\data.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/solo%20projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week%206/data.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmodin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/solo%20projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week%206/data.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/solo%20projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week%206/data.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m modin_data \u001b[39m=\u001b[39m md\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mE:/solo projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week 6/Books_rating.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/solo%20projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week%206/data.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/solo%20projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week%206/data.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRead data with Modin: \u001b[39m\u001b[39m\"\u001b[39m,(end\u001b[39m-\u001b[39mstart),\u001b[39m\"\u001b[39m\u001b[39msec\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\modin\\logging\\logger_function.py:65\u001b[0m, in \u001b[0;36mlogger_decorator.<locals>.decorator.<locals>.run_and_log\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mAny\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mif\u001b[39;00m LogMode\u001b[39m.\u001b[39mget() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdisable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     67\u001b[0m logger \u001b[39m=\u001b[39m get_logger()\n\u001b[0;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\modin\\pandas\\io.py:140\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    138\u001b[0m _, _, _, f_locals \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39mgetargvalues(inspect\u001b[39m.\u001b[39mcurrentframe())\n\u001b[0;32m    139\u001b[0m kwargs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m f_locals\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m _pd_read_csv_signature}\n\u001b[1;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m _read(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\modin\\pandas\\io.py:57\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     45\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m    Read csv file from local disk.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m    modin.pandas.DataFrame\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     Engine\u001b[39m.\u001b[39;49msubscribe(_update_engine)\n\u001b[0;32m     58\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmodin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexecution\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdispatching\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfactories\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdispatcher\u001b[39;00m \u001b[39mimport\u001b[39;00m FactoryDispatcher\n\u001b[0;32m     60\u001b[0m     squeeze \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39msqueeze\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\modin\\config\\pubsub.py:213\u001b[0m, in \u001b[0;36mParameter.subscribe\u001b[1;34m(cls, callback)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mAdd `callback` to the `_subs` list and then execute it.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39m    Callable to execute.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_subs\u001b[39m.\u001b[39mappend(callback)\n\u001b[1;32m--> 213\u001b[0m callback(\u001b[39mcls\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\modin\\pandas\\__init__.py:122\u001b[0m, in \u001b[0;36m_update_engine\u001b[1;34m(publisher)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m publisher\u001b[39m.\u001b[39mget() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRay\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    121\u001b[0m     \u001b[39mif\u001b[39;00m _is_first_update\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mRay\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 122\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mmodin\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexecution\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m initialize_ray\n\u001b[0;32m    124\u001b[0m         initialize_ray()\n\u001b[0;32m    125\u001b[0m \u001b[39melif\u001b[39;00m publisher\u001b[39m.\u001b[39mget() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNative\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    126\u001b[0m     \u001b[39m# With OmniSci storage format there is only a single worker per node\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[39m# and we allow it to work on all cores.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\modin\\core\\execution\\ray\\common\\__init__.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Licensed to Modin Development Team under one or more contributor license agreements.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# See the NOTICE file distributed with this work for additional information regarding\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# copyright ownership.  The Modin Development Team licenses this file to you under the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# ANY KIND, either express or implied. See the License for the specific language\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# governing permissions and limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m\"\"\"Common utilities for Ray execution engine.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtask_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m RayTask, SignalActor\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m initialize_ray\n\u001b[0;32m     19\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     20\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minitialize_ray\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mRayTask\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSignalActor\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\modin\\core\\execution\\ray\\common\\task_wrapper.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mThe module with helper mixin for executing functions remotely.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mTo be used as a piece of building a Ray-based engine.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39masyncio\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mray\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m@ray\u001b[39m\u001b[39m.\u001b[39mremote\n\u001b[0;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_deploy_ray_func\u001b[39m(func, args):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m    Wrap `func` to ease calling it remotely.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m        Ray identifier of the result being put to Plasma store.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ray'"
     ]
    }
   ],
   "source": [
    "os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "import modin.pandas as md\n",
    "\n",
    "start = time.time()\n",
    "modin_data = md.read_csv('E:/solo projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week 6/Books_rating.csv')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Read data with Modin: \",(end-start),\"sec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask is better than Pandas, Modin and ray with least reading time of 0.010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remove underscores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dask_data.columns=dask_data.columns.str.replace('[_]','')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Title', 'Price', 'Userid', 'profileName', 'review/helpfulness',\n",
       "       'review/score', 'review/time', 'review/summary', 'review/text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create YAML file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File Reading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Testutility.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Testutility.py\n",
    "import yaml\n",
    "import logging\n",
    "import subprocess\n",
    "import yaml\n",
    "import datetime\n",
    "import gc\n",
    "import re \n",
    "import pandas as pd\n",
    "def read_config_file(filepath):\n",
    "    with open(filepath, 'r') as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            logging.error(exc)\n",
    "\n",
    "\n",
    "def replacer(string, char):\n",
    "    pattern = char + '{2,}'\n",
    "    string = re.sub(pattern, char, string)\n",
    "    return string\n",
    "\n",
    "\n",
    "def col_header_val(df, table_config):\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace('[^\\w]', '_', regex=True)\n",
    "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
    "    df.columns = list(map(lambda x: replacer(x, '_'), list(df.columns)))\n",
    "    expected_col = list(map(lambda x: x.lower(),  table_config['columns']))\n",
    "    expected_col.sort()\n",
    "    df.columns = list(map(lambda x: x.lower(), list(df.columns)))\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    if len(df.columns) == len(expected_col) and list(expected_col)  == list(df.columns):\n",
    "        print(\"column name and column length validation passed\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"column name and column length validation failed\")\n",
    "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
    "        print(\"Following File columns are not in the YAML file\", mismatched_columns_file)\n",
    "        missing_YAML_file = list(set(expected_col).difference(df.columns))\n",
    "        print(\"Following YAML columns are not in the file uploaded\", missing_YAML_file)\n",
    "        logging.info(f'df columns: {df.columns}')\n",
    "        logging.info(f'expected columns: {expected_col}')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting books.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile books.yaml\n",
    "file_type: csv\n",
    "dataset_name: file\n",
    "file_name: books_rating\n",
    "table_name : edsurv\n",
    "inbound_delimiter : \",\"\n",
    "outbound_delimiter : \"|\"\n",
    "skip_leading_rows: 1\n",
    "columns: \n",
    "    - Id\n",
    "    - Title\n",
    "    - Price\n",
    "    - profileName\n",
    "    - review/helpfulness\n",
    "    - review/score\n",
    "    - review/time\n",
    "    - review/summary\n",
    "    - review/text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Testutility as util\n",
    "\n",
    "data_config =util.read_config_file(\"books.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data of config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_type': 'csv',\n",
       " 'dataset_name': 'file',\n",
       " 'file_name': 'books_rating',\n",
       " 'table_name': 'edsurv',\n",
       " 'inbound_delimiter': ',',\n",
       " 'outbound_delimiter': '|',\n",
       " 'skip_leading_rows': 1,\n",
       " 'columns': ['Id',\n",
       "  'Title',\n",
       "  'Price',\n",
       "  'profileName',\n",
       "  'review/helpfulness',\n",
       "  'review/score',\n",
       "  'review/time',\n",
       "  'review/summary',\n",
       "  'review/text']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal reading process of the file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dask.dataframe as dd\n",
    "df= dd.read_csv(\"books_rating.csv\",delimiter=',')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the file using config file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_type = data_config['file_type']\n",
    "source_file = \"./\" + data_config['file_name'] + f'.{file_type}'\n",
    "df_data = pd.read_csv(source_file,data_config['inbound_delimiter'])\n",
    "df_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate the header of the file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "util.col_header_val(df,data_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"columns of files are:\" ,df.columns)\n",
    "print(\"columns of YAML are:\" ,data_config['columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if util.col_header_val(df,data_config)==0:\n",
    "    print(\"validation failed\")\n",
    "else:\n",
    "    print(\"col validation passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gzip\n",
    "\n",
    "from dask import dataframe as dd\n",
    "df = dd.read_csv('E:/solo projects/Data_Glacier_virtual_internship/Data_Glacier_virtual_internship/Week 6/Books_rating.csv',delimiter=',')\n",
    "\n",
    "# Write csv in gz format in pipe separated text file (|)\n",
    "df.to_csv('Books_rating.csv.gz',\n",
    "          sep='|',\n",
    "          header=True,\n",
    "          index=False,\n",
    "          quoting=csv.QUOTE_ALL,\n",
    "          compression='gzip',\n",
    "          quotechar='\"',\n",
    "          doublequote=True,\n",
    "          line_terminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**number of files in gz format folder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**size of the gz format folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partitions = os.listdir('used_carsnew.csv.gz/')\n",
    "for partition in partitions:\n",
    "    print(partition)\n",
    "\n",
    "\n",
    "os.path.getsize('used_carsnew.csv.gz')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9cff5a362bc38ef45d817ae74b1af54d6a076e3d773891282bce078b815ba34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
